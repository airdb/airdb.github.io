(window.webpackJsonp=window.webpackJsonp||[]).push([[111],{217:function(t,n,s){"use strict";s.r(n);var i=s(0),e=Object(i.a)({},(function(){var t=this,n=t.$createElement,s=t._self._c||n;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"机器学习文档"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#机器学习文档"}},[t._v("#")]),t._v(" 机器学习文档")]),t._v(" "),s("h2",{attrs:{id:"集成学习方法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#集成学习方法"}},[t._v("#")]),t._v(" 集成学习方法")]),t._v(" "),s("pre",[s("code",[t._v("决策树算法总结\nhttps://mp.weixin.qq.com/s/tevVm0jlS6vZ3LCnczWD0w\n集成学习原理总结\nhttps://mp.weixin.qq.com/s/i_Y9r4PM-xVEQ7SncRfE_g\n随机森林算法总结\nhttps://mp.weixin.qq.com/s/bSNAN0Ki4xizKbseWzrSxg\n随机森林算法参数解释及调优\nhttps://mp.weixin.qq.com/s/hiyjuCWSCOzdF55DU5XkUw\nAdaBoost算法总结(一)\nhttps://mp.weixin.qq.com/s/Q4az1uFIEmjonKfu_eEmAQ\nAdaBoost算法总结(二)\nhttps://mp.weixin.qq.com/s/z34fVtc-4eI_b64JUF9_fg\nAdaBoost项目实战：参数择优与泛化能力\nhttps://mp.weixin.qq.com/s/7MzoixbE8rt9Y6oTV4o0ig\n梯度提升树算法原理小结\nhttps://mp.weixin.qq.com/s/cRKkvicgFcYkSYQqrBIIpQ\nscikit-learn 梯度提升树(GBDT)算法实战\nhttps://mp.weixin.qq.com/s/dS2kQ_JjHsvJyKWFrKd6aQ\nXGBoost算法原理小结\nhttps://mp.weixin.qq.com/s/wDPu_nUYODwSRwJJa8nouw\nXGBoost之切分点算法\nhttps://mp.weixin.qq.com/s/v9yF5KK2eVJOQ5UOYx4jPA\n")])]),t._v(" "),s("h2",{attrs:{id:"支持向量机"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#支持向量机"}},[t._v("#")]),t._v(" 支持向量机")]),t._v(" "),s("pre",[s("code",[t._v("浅析感知机学习算法\nhttps://mp.weixin.qq.com/s/PxL-glGrOI4glCQbwzSRXQ\n支持向量机(一)：支持向量机的分类思想\nhttps://mp.weixin.qq.com/s/cKQYk3817cnlWMVGLt8tLw\n支持向量机(二)：算法详细解析\nhttps://mp.weixin.qq.com/s/x2UGq0Zft0dxhFqfA6I1Lw\n支持向量机(三)：图解KKT条件和拉格朗日乘子法\nhttps://mp.weixin.qq.com/s/5V2Nx3c0ewTIoZtFkN0q2g\n深入浅出核函数\nhttps://mp.weixin.qq.com/s/fbuGqAfPo9uRSLctJSyDow\n支持向量机：SMO算法剖析\nhttps://mp.weixin.qq.com/s/s0d7ZoQOjtpWKlrfbVfVmA\n支持向量机应用：人脸识别\nhttps://mp.weixin.qq.com/s/g04ci5LfhL73XopcGLQv9g\n")])]),t._v(" "),s("h2",{attrs:{id:"线性回归与分类模型"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#线性回归与分类模型"}},[t._v("#")]),t._v(" 线性回归与分类模型")]),t._v(" "),s("pre",[s("code",[t._v("线性回归：不能忽视的三个问题\nhttps://mp.weixin.qq.com/s/cL7xzaofVsocqXc54eYP8g\n深入理解线性回归算法(一)\nhttps://mp.weixin.qq.com/s/mMhhyvlbt2t6ypbU-JN9mg\n深入理解线性回归算法(二)：正则项的详细分析\nhttps://mp.weixin.qq.com/s/RJi6PKAfTSIkVrd8kNZDEw\n线性分类模型(一)：线性判别模型分析\nhttps://mp.weixin.qq.com/s/3Owz3z64kMDiHP9WLd6i2A\n线性分类模型(二)：logistic回归模型分析\nhttps://mp.weixin.qq.com/s/owT0BubsfAYpjmZZTAXblQ\n比较全面的L1和L2正则化解释\nhttps://mp.weixin.qq.com/s/OWQU9jM-ZItcy1antSwZxw\n正则化方法小结\nhttps://mp.weixin.qq.com/s/CDMBQPgzcrjbZ_sX01q2hQ\n")])]),t._v(" "),s("h2",{attrs:{id:"贝叶斯思想"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#贝叶斯思想"}},[t._v("#")]),t._v(" 贝叶斯思想")]),t._v(" "),s("pre",[s("code",[t._v("浅谈频率学派和贝叶斯学派\nhttps://mp.weixin.qq.com/s/wuHq99vDNzI0Lx3eIqXAbw\n浅谈先验分布和后验分布\nhttps://mp.weixin.qq.com/s/J6mBP8Fa6XOSITM1pXNN4w\n贝叶斯分析：抛硬币的概率真的是1/2吗\nhttps://mp.weixin.qq.com/s/qzklm0v6CsV4mYvLLlyWZw\n")])]),t._v(" "),s("h2",{attrs:{id:"机器学习基础与模型评估方法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#机器学习基础与模型评估方法"}},[t._v("#")]),t._v(" 机器学习基础与模型评估方法")]),t._v(" "),s("pre",[s("code",[t._v("从机器学习谈起\nhttps://mp.weixin.qq.com/s/ahFjcIrsxNZ37USV-jFdEQ\n机器学习概论\nhttps://mp.weixin.qq.com/s/dB10CjlNVMkP6rYg7_CVSw\n机器学习算法常用指标总结\nhttps://mp.weixin.qq.com/s/u0sAIXYDC1_8JkvXu7_ozw\n模型优化的风向标:偏差与方差\nhttps://mp.weixin.qq.com/s/OvXcU7GO7Td8r8pWQ9NBNQ\n机器学习模型评估方法\nhttps://mp.weixin.qq.com/s/Vlxq2dd2q0QJdAjmSl0tQg\n机器学习模型性能评估(一)：错误率与精度\nhttps://mp.weixin.qq.com/s/Dq_87Lrd_KA63szbjFsnbQ\n机器学习模型性能评估(二)：P-R曲线和ROC曲线\nhttps://mp.weixin.qq.com/s/cr142o9DZ7KCYuxGPQNQRA\n机器学习模型性能评估(三)：代价曲线\nhttps://mp.weixin.qq.com/s/4RMrcYiZ7OGKLH5p6u3qnw\n")])]),t._v(" "),s("h2",{attrs:{id:"机器学习预处理"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#机器学习预处理"}},[t._v("#")]),t._v(" 机器学习预处理")]),t._v(" "),s("pre",[s("code",[t._v("非参数性的正态检验\nhttps://mp.weixin.qq.com/s/dcS-usNqPetmokFXc-oTvQ\n偏度与峰度的正态性分布判断\nhttps://mp.weixin.qq.com/s/VgwRuEIyvsC5K8dtgIjDuQ\n基于Q-Q图的正态性分布\nhttps://mp.weixin.qq.com/s/_UTKNcOgKQcCogk2C2tsQQ\n")])]),t._v(" "),s("h2",{attrs:{id:"神经网络"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#神经网络"}},[t._v("#")]),t._v(" 神经网络")]),t._v(" "),s("pre",[s("code",[t._v("浅谈logistic函数和softmax函数\nhttps://mp.weixin.qq.com/s/QHk0E9rdZ6wr5q8ZG00TnA\n神经网络浅讲：从神经元到深度学习(一)\nhttps://mp.weixin.qq.com/s/neGvg57iWwVfrvoS60HXnQ\n神经网络浅讲：从神经元到深度学习(二)\nhttps://mp.weixin.qq.com/s/fKpq-wxW2OJVFPfuBeewew\n")])]),t._v(" "),s("h2",{attrs:{id:"机器学习数学"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#机器学习数学"}},[t._v("#")]),t._v(" 机器学习数学")]),t._v(" "),s("pre",[s("code",[t._v("常见的几种最优化方法\nhttps://mp.weixin.qq.com/s/OTM6hapEWblwRQJJqyN-0Q\n拉格朗日乘数法\nhttps://mp.weixin.qq.com/s/PQXr5WZ8cnOLOGOOZCZcSg\n为什么梯度是函数变化最快的方向\nhttps://mp.weixin.qq.com/s/2E3LRtWPlxgpRRARVpWa_w\n梯度下降法的三种形式BGD、SGD以及MBGD\nhttps://mp.weixin.qq.com/s/dWEQTSKn28ySKf8rs5hnmA\n为什么要对数据进行归一化处理\nhttps://mp.weixin.qq.com/s/3yGKW1DIAlzzrKCR0U9eag\n机器学习中的相似性度量总结\nhttps://mp.weixin.qq.com/s/I1ovA7e98sLZHX0RVihmyA\n")])]),t._v(" "),s("h2",{attrs:{id:"k近邻算法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#k近邻算法"}},[t._v("#")]),t._v(" K近邻算法")]),t._v(" "),s("pre",[s("code",[t._v("K近邻算法(KNN)原理小结\nhttps://mp.weixin.qq.com/s/5EL3Q85v4Bo1ewnA-ZDbIQ\n")])]),t._v(" "),s("h2",{attrs:{id:"机器学习资源"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#机器学习资源"}},[t._v("#")]),t._v(" 机器学习资源")]),t._v(" "),s("pre",[s("code",[t._v("来看看这20个顶尖的开源项目\nhttps://mp.weixin.qq.com/s/yYBSDxGNa4VVs0HrNThM4A\n")])])])}),[],!1,null,null,null);n.default=e.exports}}]);